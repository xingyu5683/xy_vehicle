import os
import sys
import time
import random
import numpy as np
import argparse
import logging
import pickle
import torch
import torchvision
from distutils.util import strtobool
from threading import Thread
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter
from simulation.connection import ClientConnection
from simulation.environment import CarlaEnvironment
from networks.off_policy.ddqn.agent import DQNAgent
from encoder_init import EncodeState
from parameters import *
import torchvision.models as models
from torchvision.models import ResNet34_Weights

# -------------------- æ–°å¢ï¼šå¯¼å…¥æ¨¡ä»¿å­¦ä¹ æ¨¡å‹ç›¸å…³ --------------------
import torchvision.transforms as transforms
from PIL import Image
# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ  deque å¯¼å…¥
from collections import deque


# å®šä¹‰æ¨¡ä»¿å­¦ä¹ æ¨¡å‹ç»“æ„ï¼ˆå¿…é¡»ä¸ä½ çš„é¢„è®­ç»ƒæ¨¡å‹å®Œå…¨ä¸€è‡´ï¼‰
class ResNet34(torch.nn.Module):
    def __init__(self):
        super(ResNet34, self).__init__()
        # ä½¿ç”¨torchvision0.12.0çš„ResNet34ï¼ˆPython3.7å…¼å®¹ï¼‰
        self.resnet = models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)
        self.resnet.fc = torch.nn.Identity()  # ç§»é™¤åŸå…¨è¿æ¥å±‚

        # è¿åŠ¨çŠ¶æ€åˆ†æ”¯ - ä½¿ç”¨ä¸è®­ç»ƒä»£ç ä¸€è‡´çš„åç§°ï¼škinematics_fc
        self.kinematics_fc = torch.nn.Sequential(
            torch.nn.Linear(2, 32),
            torch.nn.ReLU(),
            torch.nn.BatchNorm1d(32)
        )

        # èåˆå±‚ - ä½¿ç”¨ä¸è®­ç»ƒä»£ç ä¸€è‡´çš„ç‹¬ç«‹å±‚åç§°ï¼Œè€ŒéSequentialå®¹å™¨
        self.fc1 = torch.nn.Linear(512 + 32, 256)
        self.fc1_bn = torch.nn.BatchNorm1d(256)
        self.fc2 = torch.nn.Linear(256, 128)
        self.fc2_bn = torch.nn.BatchNorm1d(128)
        self.relu = torch.nn.ReLU()

        # åŠ¨ä½œè¾“å‡ºå±‚ - ä½¿ç”¨ä¸è®­ç»ƒä»£ç ä¸€è‡´çš„åç§°ï¼šfc3
        self.fc3 = torch.nn.Linear(128, 3)

    def forward(self, x, kinematics):
        """
        å‰å‘ä¼ æ’­å‡½æ•°
        Args:
            x: å›¾åƒè¾“å…¥ï¼Œshape [batch_size, 3, H, W]
            kinematics: è¿åŠ¨çŠ¶æ€è¾“å…¥ï¼Œshape [batch_size, 2]
        Returns:
            control: æ§åˆ¶è¾“å‡ºï¼Œshape [batch_size, 3]ï¼Œå…¶ä¸­ï¼š
                     - å‰ä¸¤åˆ—ï¼šæ²¹é—¨/åˆ¹è½¦ï¼ŒèŒƒå›´0~1
                     - ç¬¬ä¸‰åˆ—ï¼šæ–¹å‘ç›˜ï¼ŒèŒƒå›´-1~1
        """
        # å›¾åƒç‰¹å¾æå–
        img_feat = self.resnet(x)  # shape: [batch_size, 512]

        # è¿åŠ¨çŠ¶æ€ç‰¹å¾æå–
        kin_feat = self.kinematics_fc(kinematics)  # shape: [batch_size, 32]

        # ç‰¹å¾èåˆ
        fusion_feat = torch.cat([img_feat, kin_feat], dim=1)  # shape: [batch_size, 544]
        fusion_feat = self.relu(self.fc1_bn(self.fc1(fusion_feat)))  # shape: [batch_size, 256]
        fusion_feat = self.relu(self.fc2_bn(self.fc2(fusion_feat)))  # shape: [batch_size, 128]

        # åŠ¨ä½œè¾“å‡º
        control = self.fc3(fusion_feat)  # shape: [batch_size, 3]

        # æ¿€æ´»å‡½æ•°å¤„ç†
        # æ²¹é—¨/åˆ¹è½¦ï¼šsigmoidæ¿€æ´»ï¼ŒèŒƒå›´0~1
        # æ–¹å‘ç›˜ï¼štanhæ¿€æ´»ï¼ŒèŒƒå›´-1~1
        throttle_brake = torch.sigmoid(control[:, :2])  # shape: [batch_size, 2]
        steering = torch.tanh(control[:, 2:3])  # shape: [batch_size, 1]

        # æ‹¼æ¥å®Œæ•´æ§åˆ¶è¾“å‡º
        control = torch.cat([throttle_brake, steering], dim=1)  # shape: [batch_size, 3]

        return control


# æ ¸å¿ƒä¿®æ”¹1ï¼šé€‚é…ç¯å¢ƒçš„5ä¸ªç¦»æ•£åŠ¨ä½œç©ºé—´ï¼ˆ-0.5/-0.3/0.0/0.3/0.5è½¬å‘ï¼‰
def continuous_to_discrete(continuous_action, num_actions=5):
    """
    å°†è¿ç»­åŠ¨ä½œè½¬æ¢ä¸ºç¦»æ•£åŠ¨ä½œï¼ˆé€‚é…ç¯å¢ƒçš„5ä¸ªåŠ¨ä½œç´¢å¼•ï¼‰
    Args:
        continuous_action: è¿ç»­åŠ¨ä½œï¼Œshape [3]ï¼ˆæ²¹é—¨ã€åˆ¹è½¦ã€è½¬å‘ï¼‰
        num_actions: ç¦»æ•£åŠ¨ä½œæ•°é‡ï¼ˆå›ºå®šä¸º5ï¼Œå¯¹åº”ç¯å¢ƒçš„5ä¸ªåŠ¨ä½œï¼‰
    Returns:
        discrete_action: ç¦»æ•£åŠ¨ä½œç´¢å¼•ï¼ˆ0~4ï¼‰
    """
    # 1. æå–è½¬å‘è§’å¹¶æ·»åŠ æ­»åŒºé˜ˆå€¼
    steer = continuous_action[2]  # è½¬å‘è§’ï¼ŒèŒƒå›´-1~1
    dead_zone = 0.1  # æ­»åŒºé˜ˆå€¼ï¼šç»å¯¹å€¼å°äº0.1çš„è½¬å‘è§’è§†ä¸ºç›´è¡Œ
    if abs(steer) < dead_zone:
        steer = 0.0  # è½»å¾®è½¬å‘ç›´æ¥ç½®ä¸º0ï¼Œå¼ºåˆ¶ç›´è¡Œ

    # 2. æ˜ å°„åˆ°5ä¸ªåŠ¨ä½œç´¢å¼•ï¼ˆé€‚é…ç¯å¢ƒçš„åŠ¨ä½œç©ºé—´é¡ºåºï¼‰
    # ç¯å¢ƒåŠ¨ä½œç©ºé—´é¡ºåºï¼š0(-0.5), 1(-0.3), 2(0.0), 3(0.3), 4(0.5)
    if steer <= -0.4:
        discrete_action = 0  # å¤§å·¦è½¬
    elif steer <= -0.2:
        discrete_action = 1  # ä¸­å·¦è½¬
    elif abs(steer) < 0.2:
        discrete_action = 2  # ç›´è¡Œï¼ˆæ ¸å¿ƒï¼‰
    elif steer <= 0.4:
        discrete_action = 3  # ä¸­å³è½¬
    else:
        discrete_action = 4  # å¤§å³è½¬

    # 3. ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
    discrete_action = max(0, min(num_actions - 1, discrete_action))
    return discrete_action


# å›¾åƒé¢„å¤„ç†ï¼ˆä¸æ¨¡ä»¿å­¦ä¹ ä¸€è‡´ï¼‰
def get_imitation_transform():
    return transforms.Compose([
        transforms.Resize((480, 640)),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=(0.1840, 0.1659, 0.1613),
            std=(0.2540, 0.2386, 0.2599)
        )
    ])


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--exp-name', type=str, default='ddqn', help='name of the experiment')
    parser.add_argument('--env-name', type=str, default='carla', help='name of the simulation environment')
    parser.add_argument('--learning-rate', type=float, default=DQN_LEARNING_RATE, help='learning rate of the optimizer')
    parser.add_argument('--seed', type=int, default=SEED, help='seed of the experiment')
    parser.add_argument('--total-episodes', type=int, default=EPISODES, help='total timesteps of the experiment')
    parser.add_argument('--train', type=bool, default=True, help='is it training?')
    parser.add_argument('--town', type=str, default="Town02", help='which town do you like?')
    parser.add_argument('--load-checkpoint', type=bool, default=MODEL_LOAD, help='resume training?')
    # æ ¸å¿ƒä¿®æ”¹2ï¼šé»˜è®¤åŠ¨ä½œæ•°æ”¹ä¸º5ï¼ˆé€‚é…ç¯å¢ƒçš„5ä¸ªåŠ¨ä½œç©ºé—´ï¼‰
    parser.add_argument('--num-actions', type=int, default=5, help='num of discrete actions')
    parser.add_argument('--torch-deterministic', type=lambda x: bool(strtobool(x)), default=True, nargs='?', const=True,
                        help='if toggled, `torch.backends.cudnn.deterministic=False`')
    parser.add_argument('--cuda', type=lambda x: bool(strtobool(x)), default=True, nargs='?', const=True,
                        help='if toggled, cuda will not be enabled by deafult')
    parser.add_argument('--imitation-ckpt', type=str, default='',
                        help='path to imitation learning checkpoint')
    parser.add_argument('--convergence-episodes', type=int, default=50,
                        help='è¿ç»­å¤šå°‘ä¸ªepisodeå¥–åŠ±ç¨³å®šåˆ™è®¤ä¸ºæ”¶æ•›')
    parser.add_argument('--convergence-threshold', type=float, default=0.05,
                        help='å¥–åŠ±æ³¢åŠ¨é˜ˆå€¼ï¼ˆæ–¹å·®/å‡å€¼ < é˜ˆå€¼åˆ™è®¤ä¸ºç¨³å®šï¼‰')
    parser.add_argument('--min-episodes', type=int, default=200,
                        help='æœ€å°è®­ç»ƒepisodeæ•°ï¼ˆé¿å…è¿‡æ—©åœæ­¢ï¼‰')

    args = parser.parse_args()
    return args


def runner():
    # ========================================================================
    #                           BASIC PARAMETER & LOGGING SETUP
    # ========================================================================

    args = parse_args()
    exp_name = args.exp_name
    town = args.town
    train = args.train
    checkpoint_load = args.load_checkpoint
    # æ ¸å¿ƒä¿®æ”¹3ï¼šåŠ¨ä½œæ•°ä»å‚æ•°è¯»å–ï¼ˆé»˜è®¤5ï¼‰
    num_actions = args.num_actions

    reward_history = deque(maxlen=args.convergence_episodes)
    is_converged = False

    imitation_ckpt = args.imitation_ckpt

    try:
        if exp_name == 'ddqn':
            run_name = f"DDQN"
    except Exception as e:
        print(e)
        sys.exit()

    if train == True:
        writer = SummaryWriter(f"runs/{run_name}/{town}")
    else:
        writer = SummaryWriter(f"runs/{run_name}_TEST/{town}")
    writer.add_text(
        "hyperparameters",
        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}" for key, value in vars(args).items()])))

    # Seeding to reproduce the results
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.backends.cudnn.deterministic = args.torch_deterministic

    # æ ¸å¿ƒä¿®æ”¹4ï¼šn_actionsæ”¹ä¸ºnum_actionsï¼ˆ5ä¸ªï¼‰
    n_actions = num_actions  # ç°åœ¨å›ºå®šä¸º5ä¸ªåŠ¨ä½œï¼š0(-0.5),1(-0.3),2(0.0),3(0.3),4(0.5)
    epoch = 0
    cumulative_score = 0
    episodic_length = list()
    scores = list()
    deviation_from_center = 0
    distance_covered = 0

    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # ========================================================================
    #                           CREATING THE SIMULATION
    # ========================================================================
    try:
        client, world = ClientConnection(town).setup()
        logging.info("Connection has been setup successfully.")
    except Exception as e:
        logging.error(f"Connection has been refused by the server: {e}")
        sys.exit()
    if train:
        # æ ¸å¿ƒä¿®æ”¹5ï¼šç¡®è®¤ç¯å¢ƒä½¿ç”¨ç¦»æ•£åŠ¨ä½œï¼ˆcontinuous_action=Falseï¼‰
        env = CarlaEnvironment(client, world, town, continuous_action=False, algorithm='dqn', route_mode='1')
    else:
        env = CarlaEnvironment(client, world, town, checkpoint_frequency=None, continuous_action=False, algorithm='dqn',
                               route_mode='1')
    encode = EncodeState(LATENT_DIM)

    time.sleep(0.5)

    # ========================================================================
    # æ ¸å¿ƒä¿®æ”¹ï¼šè®¡ç®—ç¼–ç åçš„çŠ¶æ€ç»´åº¦ï¼ˆé€‚é…å¼¯é“è·ç¦»ç‰¹å¾ï¼‰
    # ========================================================================
    sample_observation = env.reset()
    sample_encoded = encode.process(sample_observation)
    state_dim = len(sample_encoded)
    print(f"\nâœ… ç¼–ç åçš„çŠ¶æ€ç»´åº¦ï¼š{state_dim}ï¼ˆåŒ…å«å¼¯é“è·ç¦»ç‰¹å¾ï¼‰")
    print(f"âœ… ç¦»æ•£åŠ¨ä½œæ•°é‡ï¼š{n_actions}ï¼ˆé€‚é…ç¯å¢ƒçš„5ä¸ªåŠ¨ä½œç©ºé—´ï¼‰")

    # ========================================================================
    #                           ALGORITHM
    # ========================================================================
    if train is False:  # Test
        # æ ¸å¿ƒä¿®æ”¹6ï¼šä¼ å…¥æ­£ç¡®çš„åŠ¨ä½œæ•°ï¼ˆ5ï¼‰å’ŒçŠ¶æ€ç»´åº¦
        agent = DQNAgent(town, n_actions, state_dim=state_dim)
        agent.load_model()
        for params in agent.q_network_eval.parameters():
            params.requires_grad = False
        for params in agent.q_network_target.parameters():
            params.requires_grad = False
    else:  # Training
        if checkpoint_load:
            agent = DQNAgent(town, n_actions, state_dim=state_dim)
            agent.load_model()
        else:
            agent = DQNAgent(town, n_actions, state_dim=state_dim)

    # -------------------- ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ æ¨¡å‹å¡«å……ç»éªŒæ±  --------------------
    if exp_name == 'ddqn' and not checkpoint_load:
        print(f"\n{'=' * 50}")
        print(f"å¼€å§‹ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ æ¨¡å‹å¡«å……ç»éªŒæ± ...")
        print(f"æ¨¡ä»¿å­¦ä¹ æƒé‡è·¯å¾„ï¼š{imitation_ckpt}")

        # 1. åŠ è½½æ¨¡ä»¿å­¦ä¹ æ¨¡å‹
        imitation_model = ResNet34().to(device)
        try:
            checkpoint = torch.load(imitation_ckpt, map_location=device)
            if 'model_state_dict' in checkpoint:
                imitation_model.load_state_dict(checkpoint['model_state_dict'])
            else:
                imitation_model.load_state_dict(checkpoint)
            imitation_model.eval()
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡ä»¿å­¦ä¹ æ¨¡å‹")
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡ä»¿å­¦ä¹ æ¨¡å‹å¤±è´¥ï¼š{e}ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œå¡«å……ç»éªŒæ± ")
            # æ ¸å¿ƒä¿®æ”¹7ï¼šéšæœºåŠ¨ä½œèŒƒå›´æ”¹ä¸º0~4ï¼ˆ5ä¸ªåŠ¨ä½œï¼‰
            while agent.replay_buffer.counter < agent.replay_buffer.buffer_size:
                observation = env.reset()
                observation_encoded = encode.process(observation)
                done = False
                while not done:
                    action = random.randint(0, n_actions - 1)  # 0~4
                    new_observation, reward, done, _ = env.step(action)
                    if new_observation is None:
                        break
                    new_observation_encoded = encode.process(new_observation)
                    agent.save_transition(observation_encoded, action, reward, new_observation_encoded, int(done))
                    observation_encoded = new_observation_encoded
            print(f"âœ… å·²ä½¿ç”¨éšæœºåŠ¨ä½œå¡«å……ç»éªŒæ± ")
        else:
            # 2. å‡†å¤‡å›¾åƒé¢„å¤„ç†
            transform = get_imitation_transform()

            # 3. ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ æ¨¡å‹å¡«å……ç»éªŒæ± 
            filled_count = 0
            max_filled = agent.replay_buffer.buffer_size
            while filled_count < max_filled:
                observation = env.reset()
                done = False
                while not done and filled_count < max_filled:
                    try:
                        raw_image, nav_features = observation
                        velocity_kmh = nav_features[1]
                        velocity = velocity_kmh / 3.6
                        waypoint_dist = nav_features[5]
                        kinematics_np = np.array([velocity, waypoint_dist], dtype=np.float32)
                        kinematics_tensor = torch.tensor(kinematics_np, dtype=torch.float32).unsqueeze(0).to(device)
                    except Exception as e:
                        print(f"è§£æç¯å¢ƒè¾“å‡ºå¤±è´¥ï¼š{e}ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œ")
                        action = random.randint(0, n_actions - 1)  # 0~4
                    else:
                        # 4. ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ æ¨¡å‹ç”Ÿæˆè¿ç»­åŠ¨ä½œ
                        with torch.no_grad():
                            image_pil = Image.fromarray(raw_image.astype(np.uint8))
                            image_tensor = transform(image_pil).unsqueeze(0).to(device)
                            continuous_action = imitation_model(image_tensor, kinematics_tensor).cpu().numpy()[0]
                            # æ ¸å¿ƒä¿®æ”¹8ï¼šè½¬æ¢ä¸º5ä¸ªç¦»æ•£åŠ¨ä½œç´¢å¼•
                            action = continuous_to_discrete(continuous_action, num_actions=n_actions)
                            print(f"æ¨¡å‹è¾“å‡º | è½¬å‘: {continuous_action[2]:.3f} â†’ åŠ¨ä½œç´¢å¼•: {action}")

                    # æ‰§è¡ŒåŠ¨ä½œ
                    new_observation, reward, done, _ = env.step(action)
                    if new_observation is None:
                        break

                    # ä¿å­˜åˆ°ç»éªŒæ± 
                    observation_encoded = encode.process(observation)
                    new_observation_encoded = encode.process(new_observation)
                    agent.save_transition(observation_encoded, action, reward, new_observation_encoded, int(done))
                    filled_count += 1

                    # æ›´æ–°è§‚å¯Ÿ
                    observation = new_observation

                print(f"å·²å¡«å……ç»éªŒæ± ï¼š{filled_count}/{max_filled}")

            print(f"âœ… å·²ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ æ¨¡å‹å¡«å……ç»éªŒæ± ")
    elif exp_name == 'ddqn' and checkpoint_load:
        # æ ¸å¿ƒä¿®æ”¹9ï¼šéšæœºåŠ¨ä½œèŒƒå›´æ”¹ä¸º0~4
        while agent.replay_buffer.counter < agent.replay_buffer.buffer_size:
            observation = env.reset()
            observation = encode.process(observation)
            done = False
            while not done:
                action = random.randint(0, n_actions - 1)  # 0~4
                new_observation, reward, done, _ = env.step(action)
                new_observation = encode.process(new_observation)
                agent.save_transition(observation, action, reward, new_observation, int(done))
                observation = new_observation

    # -------------------- è®­ç»ƒå¾ªç¯ --------------------
    if args.train:
        for step in range(epoch + 1, EPISODES + 1):
            if is_converged:
                break

            # Reset
            done = False
            observation = env.reset()
            observation = encode.process(observation)
            current_ep_reward = 0

            # Episode start: timestamp
            t1 = datetime.now()

            while not done:
                # æ ¸å¿ƒä¿®æ”¹10ï¼šagentè¾“å‡º0~4çš„åŠ¨ä½œç´¢å¼•ï¼Œç›´æ¥ä¼ ç»™ç¯å¢ƒ
                action = agent.get_action(args.train, observation)
                # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿åŠ¨ä½œç´¢å¼•åœ¨0~4èŒƒå›´å†…
                action = max(0, min(n_actions - 1, action))
                new_observation, reward, done, info = env.step(action)

                if new_observation is None:
                    break
                new_observation = encode.process(new_observation)
                current_ep_reward += reward

                agent.save_transition(observation, action, reward, new_observation, int(done))
                if agent.get_len_buffer() > WARMING_UP:
                    agent.learn()

                observation = new_observation

            # Episode end : timestamp
            t2 = datetime.now()
            t3 = t2 - t1
            episodic_length.append(abs(t3.total_seconds()))

            deviation_from_center += info[1]
            distance_covered += info[0]

            scores.append(current_ep_reward)
            reward_history.append(current_ep_reward)

            # æ”¶æ•›åˆ¤æ–­é€»è¾‘
            if step >= args.min_episodes:
                if len(reward_history) == args.convergence_episodes:
                    mean_reward = np.mean(reward_history)
                    var_reward = np.var(reward_history)

                    if mean_reward != 0:
                        cv_reward = var_reward / abs(mean_reward)
                    else:
                        cv_reward = float('inf')

                    if cv_reward < args.convergence_threshold:
                        is_converged = True
                        print(f"\n{'=' * 60}")
                        print(f"ğŸ‰ æ¨¡å‹å·²æ”¶æ•›ï¼")
                        print(f"æ”¶æ•›æŒ‡æ ‡ï¼š")
                        print(f"- è¿ç»­ {args.convergence_episodes} ä¸ªepisodeçš„å¹³å‡å¥–åŠ±ï¼š{mean_reward:.2f}")
                        print(f"- å¥–åŠ±æ–¹å·®ï¼š{var_reward:.2f}")
                        print(f"- å˜å¼‚ç³»æ•°ï¼š{cv_reward:.4f}ï¼ˆ< {args.convergence_threshold}ï¼‰")
                        print(f"è®­ç»ƒæ€»episodeæ•°ï¼š{step}")
                        print(f"{'=' * 60}\n")

                        agent.save_model(current_ep_reward, step)
                        data_obj = {'cumulative_score': cumulative_score, 'epsilon': agent.epsilon, 'epoch': step}
                        os.makedirs(f'checkpoints/DDQN/{town}', exist_ok=True)
                        with open(f'checkpoints/DDQN/{town}/checkpoint_ddqn.pickle', 'wb') as handle:
                            pickle.dump(data_obj, handle)
                        break

            if checkpoint_load:
                cumulative_score = ((cumulative_score * (step - 1)) + current_ep_reward) / (step)
            else:
                cumulative_score = np.mean(scores)

            print('Starting Episode: ', step, ', Epsilon Now:  {:.3f}'.format(agent.epsilon),
                  'Reward:  {:.2f}'.format(current_ep_reward), ', Average Reward:  {:.2f}'.format(cumulative_score))
            agent.save_model(current_ep_reward, step)

            if step >= 10 and step % 10 == 0:
                if exp_name == 'ddqn':
                    data_obj = {'cumulative_score': cumulative_score, 'epsilon': agent.epsilon, 'epoch': step}
                    os.makedirs(f'checkpoints/DDQN/{town}', exist_ok=True)
                    with open(f'checkpoints/DDQN/{town}/checkpoint_ddqn.pickle', 'wb') as handle:
                        pickle.dump(data_obj, handle)

                writer.add_scalar("Cumulative Reward/info", cumulative_score, step)
                writer.add_scalar("Epsilon/info", agent.epsilon, step)
                writer.add_scalar("Episodic Reward/episode", scores[-1], step)
                writer.add_scalar("Average Episodic Reward/info", np.mean(scores[-10]), step)
                writer.add_scalar("Episode Length (s)/info", np.mean(episodic_length), step)
                writer.add_scalar("Average Deviation from Center/episode", deviation_from_center / 10, step)
                writer.add_scalar("Average Distance Covered (m)/episode", distance_covered / 10, step)

                episodic_length = list()
                deviation_from_center = 0
                distance_covered = 0

        print("Terminating the run.")
        sys.exit()
    else:
        # Testing
        for step in range(epoch + 1, EPISODES + 1):
            # Reset
            done = False
            observation = env.reset()
            observation = encode.process(observation)
            current_ep_reward = 0

            # Episode start: timestamp
            t1 = datetime.now()

            while not done:
                # æ ¸å¿ƒä¿®æ”¹11ï¼šæµ‹è¯•é˜¶æ®µåŒæ ·è¾“å‡º0~4çš„åŠ¨ä½œç´¢å¼•
                action = agent.get_action(args.train, observation)
                action = max(0, min(n_actions - 1, action))
                new_observation, reward, done, info = env.step(action)

                if new_observation is None:
                    break
                new_observation = encode.process(new_observation)
                current_ep_reward += reward
                observation = new_observation

            # Episode end : timestamp
            t2 = datetime.now()
            t3 = t2 - t1
            episodic_length.append(abs(t3.total_seconds()))

            deviation_from_center += info[1]
            distance_covered += info[0]

            scores.append(current_ep_reward)

            if checkpoint_load:
                cumulative_score = ((cumulative_score * (step - 1)) + current_ep_reward) / (step)
            else:
                cumulative_score = np.mean(scores)

            print('Starting Episode: ', step, ', Epsilon Now:  {:.3f}'.format(agent.epsilon),
                  'Reward:  {:.2f}'.format(current_ep_reward), ', Average Reward:  {:.2f}'.format(cumulative_score))

            writer.add_scalar("TEST: Episodic Reward/episode", scores[-1], step)
            writer.add_scalar("TEST: Cumulative Reward/info", cumulative_score, step)
            writer.add_scalar("TEST: Episode Length (s)/info", np.mean(episodic_length), step)
            writer.add_scalar("TEST: Deviation from Center/episode", deviation_from_center, step)
            writer.add_scalar("TEST: Distance Covered (m)/episode", distance_covered, step)

            episodic_length = list()
            deviation_from_center = 0
            distance_covered = 0

        print("Terminating the run.")
        sys.exit()


if __name__ == "__main__":
    runner()